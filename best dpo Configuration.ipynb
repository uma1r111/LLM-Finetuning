{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":432573,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":352649,"modelId":373928}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the latest development version of trl from GitHub\n!pip install git+https://github.com/huggingface/trl.git\n\n# Verify trl version\nimport trl\nprint(f\"trl version: {trl.__version__}\")\n\nimport torch\nimport json\nimport psutil\nimport random\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import DPOTrainer, DPOConfig\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n# Clear GPU cache at the start to free up memory from previous runs\ntorch.cuda.empty_cache()\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Memory monitoring function\ndef print_memory_footprint():\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n        gpu_memory_cached = torch.cuda.memory_reserved() / (1024 ** 3)\n        print(f\"[GPU] Memory Allocated: {gpu_memory:.2f} GB, Cached: {gpu_memory_cached:.2f} GB\")\n    else:\n        print(\"[GPU] No GPU detected.\")\n    memory = psutil.virtual_memory()\n    used_memory_gb = memory.used / (1024 ** 3)\n    total_memory_gb = memory.total / (1024 ** 3)\n    print(f\"[CPU] Memory Usage: {used_memory_gb:.2f} GB / {total_memory_gb:.2f} GB\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# Best SFT model path\nbest_sft_model_path = \"/kaggle/input/model-5/other/default/1/tinyllama-qa-exp-lowtemp\"\n\n# Set seed for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# Load Anthropic preference dataset\ndataset = load_dataset(\"Anthropic/hh-rlhf\")['train']\n\n# Select a fixed subset of 1000 examples\nsubset_size = 1000\ndataset = dataset.shuffle(seed=42).select(range(subset_size))\n\n# Preprocess dataset for DPO\ndef preprocess_preference_data(example):\n    chosen = example['chosen']\n    rejected = example['rejected']\n    prompt = chosen.rsplit(\"Assistant:\", 1)[0] + \"Assistant:\"\n    chosen_response = chosen.rsplit(\"Assistant:\", 1)[1].strip()\n    rejected_response = rejected.rsplit(\"Assistant:\", 1)[1].strip()\n    return {\n        'prompt': prompt,\n        'chosen': chosen_response,\n        'rejected': rejected_response\n    }\n\npreference_dataset = dataset.map(preprocess_preference_data)\n\n# Define evaluation prompts\neval_prompts = [\n    \"Question: What is the capital city of Japan? Answer:\",\n    \"Question: Who wrote the novel 'Pride and Prejudice'? Answer:\",\n    \"Question: What is the chemical symbol for gold? Answer:\",\n    \"Question: In which year did the Titanic sink? Answer:\",\n    \"Question: What is the largest mammal on Earth? Answer:\",\n    \"Question: Who painted the Mona Lisa? Answer:\",\n    \"Question: What is the main source of energy for Earth's climate system? Answer:\",\n    \"Question: What is the longest river in the world? Answer:\",\n    \"Question: Who discovered penicillin? Answer:\",\n    \"Question: What is the primary language spoken in Brazil? Answer:\"\n]\n\n# Function to generate responses with memory optimizations\ndef generate_responses(model_path, prompts):\n    torch.cuda.empty_cache()\n    print(\"Before loading model:\")\n    print_memory_footprint()\n    \n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            low_cpu_mem_usage=True\n        )\n        print(\"After loading model:\")\n        print_memory_footprint()\n        \n        model.eval()\n        responses = []\n        for prompt in prompts:\n            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n            with torch.no_grad():\n                with torch.autocast(device_type='cuda', dtype=torch.float16):\n                    output = model.generate(\n                        input_ids,\n                        max_new_tokens=50,\n                        pad_token_id=tokenizer.eos_token_id,\n                        temperature=0.7,\n                        do_sample=True,\n                        top_p=0.9,\n                        repetition_penalty=1.1\n                    )\n            response = tokenizer.decode(output[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n            responses.append(response)\n        \n        return responses\n    finally:\n        if 'model' in locals():\n            del model\n        torch.cuda.empty_cache()\n        print(\"After deleting model:\")\n        print_memory_footprint()\n\n# Generate responses for best SFT model\nprint(\"Generating SFT responses...\")\nsft_responses = generate_responses(best_sft_model_path, eval_prompts)\nwith open(\"sft_responses.json\", \"w\") as f:\n    json.dump({\"prompts\": eval_prompts, \"responses\": sft_responses}, f)\n\n# Define DPO configurations with beta\ndpo_configurations = [\n\n    {\n        \"name\": \"DPO_Experimental2\",\n        \"lora\": {\n            \"r\": 8,\n            \"target_modules\": [\"q_proj\", \"v_proj\"],\n            \"lora_alpha\": 16,\n            \"lora_dropout\": 0.05,\n            \"bias\": \"none\"\n        },\n        \"dpo\": {\n            \"learning_rate\": 5e-5,\n            \"batch_size\": 1,\n            \"epochs\": 2,\n            \"gradient_accumulation_steps\": 2,\n            \"beta\": 0.8\n        },\n        \"output_dir\": \"./dpo_trial5\"\n    }\n]\n\n# Train and evaluate DPO models\nfor config in dpo_configurations:\n    print(f\"\\nStarting DPO Trial: {config['name']}\")\n    print(\"Configuration:\")\n    print(json.dumps(config, indent=2))\n    print_memory_footprint()\n    \n    try:\n        # Load model with memory optimizations\n        print(\"Loading model...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            best_sft_model_path,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            low_cpu_mem_usage=True\n        )\n        \n        print(\"Applying LoRA...\")\n        lora_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM,\n            r=config[\"lora\"][\"r\"],\n            lora_alpha=config[\"lora\"][\"lora_alpha\"],\n            lora_dropout=config[\"lora\"][\"lora_dropout\"],\n            target_modules=config[\"lora\"][\"target_modules\"],\n            bias=config[\"lora\"][\"bias\"]\n        )\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n        \n        # Load reference model\n        print(\"Loading reference model...\")\n        ref_model = AutoModelForCausalLM.from_pretrained(\n            best_sft_model_path,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            low_cpu_mem_usage=True\n        )\n        \n        # Create DPO config\n        print(\"Creating DPO trainer...\")\n        dpo_config = DPOConfig(\n            output_dir=config[\"output_dir\"],\n            per_device_train_batch_size=config[\"dpo\"][\"batch_size\"],\n            num_train_epochs=config[\"dpo\"][\"epochs\"],\n            learning_rate=config[\"dpo\"][\"learning_rate\"],\n            gradient_accumulation_steps=config[\"dpo\"][\"gradient_accumulation_steps\"],\n            beta=config[\"dpo\"][\"beta\"],\n            logging_dir=f\"./logs/{config['name']}\",\n            logging_steps=10,\n            save_strategy=\"epoch\",\n            report_to=\"none\",\n            fp16=True,\n            gradient_checkpointing=True,\n            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n            optim=\"adafactor\",\n            max_grad_norm=0.3,\n            remove_unused_columns=False\n        )\n        \n        dpo_trainer = DPOTrainer(\n            model=model,\n            ref_model=ref_model,\n            args=dpo_config,\n            train_dataset=preference_dataset\n        )\n        \n        print(\"Starting training...\")\n        dpo_trainer.train()\n        \n        print(\"Saving model...\")\n        dpo_trainer.save_model(config[\"output_dir\"])\n        \n        print(\"Generating responses...\")\n        responses = generate_responses(config[\"output_dir\"], eval_prompts)\n        with open(f\"{config['output_dir']}/responses.json\", \"w\") as f:\n            json.dump({\"prompts\": eval_prompts, \"responses\": responses}, f)\n            \n        print(f\"Completed {config['name']} successfully!\")\n            \n    except Exception as e:\n        print(f\"Error during DPO training {config['name']}: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        # Clean up\n        if 'model' in locals():\n            del model\n        if 'ref_model' in locals():\n            del ref_model\n        if 'dpo_trainer' in locals():\n            del dpo_trainer\n        torch.cuda.empty_cache()\n        print(\"Memory after cleanup:\")\n        print_memory_footprint()\n\nprint(\"\\n✅ DPO training complete. Evaluate the outputs in 'sft_responses.json' and each 'dpo_trialX/responses.json'.\")","metadata":{"id":"8B66RPSF9MLv","trusted":true,"execution":{"iopub.status.busy":"2025-06-12T11:57:58.319522Z","iopub.execute_input":"2025-06-12T11:57:58.319808Z","iopub.status.idle":"2025-06-12T12:40:28.910974Z","shell.execute_reply.started":"2025-06-12T11:57:58.319784Z","shell.execute_reply":"2025-06-12T12:40:28.910276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\n\n# Define paths to the response JSON files\nsft_response_path = \"sft_responses.json\"\ndpo_response_paths = {\n    \"DPO_Experimental1\": \"./dpo_trial4/responses.json\",\n    \"DPO_Experimental2\": \"./dpo_trial5/responses.json\"\n}\n\n# Function to load and print responses from a JSON file\ndef print_responses(file_path, model_name):\n    print(f\"\\n=== Responses for {model_name} ===\")\n    try:\n        if not os.path.exists(file_path):\n            print(f\"Error: File {file_path} not found.\")\n            return\n        \n        with open(file_path, \"r\") as f:\n            data = json.load(f)\n        \n        prompts = data.get(\"prompts\", [])\n        responses = data.get(\"responses\", [])\n        \n        if not prompts or not responses or len(prompts) != len(responses):\n            print(f\"Error: Invalid or mismatched data in {file_path}.\")\n            return\n        \n        for i, (prompt, response) in enumerate(zip(prompts, responses), 1):\n            print(f\"\\nPrompt {i}: {prompt}\")\n            print(f\"Response: {response}\")\n    \n    except Exception as e:\n        print(f\"Error reading {file_path}: {str(e)}\")\n\n# Print SFT responses\nprint_responses(sft_response_path, \"Best SFT Model\")\n\n# Print DPO responses for both configurations\nfor model_name, file_path in dpo_response_paths.items():\n    print_responses(file_path, model_name)\n\nprint(\"\\n✅ Response printing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T12:46:27.346079Z","iopub.execute_input":"2025-06-12T12:46:27.346440Z","iopub.status.idle":"2025-06-12T12:46:27.355541Z","shell.execute_reply.started":"2025-06-12T12:46:27.346410Z","shell.execute_reply":"2025-06-12T12:46:27.354827Z"}},"outputs":[],"execution_count":null}]}